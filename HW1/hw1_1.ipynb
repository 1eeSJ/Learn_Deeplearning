{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:12:58.337715Z",
     "start_time": "2023-09-05T11:12:57.617134Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:13:06.986233Z",
     "start_time": "2023-09-05T11:13:06.911492Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "t1_cuda = t1.to(torch.device('cuda'))\n",
    "print(t1_cuda.device)  # >>> cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:13:33.775422Z",
     "start_time": "2023-09-05T11:13:33.751252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- 위 코드에서 torch.tensor() 함수는 주어진 리스트내 원소 타입을 그대로 유지시켜 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor & torch.tensor는 항상 주어진 data를 복사함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복사를 피하고 싶다면 numpy의 array와 torch.as_tensor를 같이 사용하면 복사를 하지않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0000e+00, 1.8750e+00, 1.5414e-44, 0.0000e+00])\n",
      "tensor([ 1.4013e-45,  0.0000e+00, -4.1276e-18,  5.2829e-43])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.ones(*size)                 => 주어진 shape으로 tensor를 만들어주는데 모두 1로 채움\n",
    "- torch.ones_like(input=tensor)     => 입력 tensor와 동일한 shape으로 만들어주는데 모두 1로 채움\n",
    "- torch.zeros(*size)                => 주어진 shape으로 tensor를 만들어주는데 모두 0으로 채움\n",
    "- torch.zeros_like(input=tensor)    => 입력 tensor와 동일한 shape으로 만들어주는데 모두 0으로 채움\n",
    "- torch.empty                       => 주어진 shape으로 tensor를 만들어주는데 모두 초기화 되지 않은 값으로 채움\n",
    "- torch.empty_like                  => 입력 tensor와 동일한 shape으로 만들어주는데 모두 초기화 되지 않은 값으로 채움\n",
    "- torch.eye(n)                      => 주어진 n으로 n by n shape의 텐서를 만드는데 대각선 값을 1, 나머지를 0으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 12]])\n",
      "tensor([[0.0453, 0.5035, 0.9978]])\n",
      "tensor([[0.7419, 0.5923, 0.2908]])\n",
      "tensor([[ 9.2270, 10.0961],\n",
      "        [ 9.4067,  9.5878],\n",
      "        [10.0763, 11.1161]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randint(low=0, high, size,...) => size= shape으로 tensor를 만드는데 low~high 사이의 랜덤 정수값을 넣어서 만듦.\n",
    "- torch.rand(size) => size= shape으로 tensor를 만들고 (0,1] 사이의 랜덤 실수값을 넣어서 만듦.\n",
    "- torch.randn(size) => 표준정규분포 X~N(0, 1)로 부터 난수를 생성해서 텐서 만듦.\n",
    "- torch.normal(mean, std, size) => 정규 분포에서 무작위 샘플을 추출하는 데 사용됨.\n",
    "    - mean : 각 출력 요소의 정규 분포의 평균\n",
    "    - std : 각 출력 요소의 정규 분포의 표준편차\n",
    "- torch.linspace(start, end, steps) => linearly space vectors의 줄인말, start~end에 대해 동일한 간격으로 특정 갯수만큼 vector 생성\n",
    "- torch.arange(n) => 기본 start=0, steps=1로 특정 갯수(N) 만큼 vector 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.manual_seed(seed) => 난수생성기의 시드를 고정된 값으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[11.0289,  9.8471, 15.3141],\n",
      "        [16.7113,  4.7063,  7.4425]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.ones 의 타입은 float32\n",
    "- torch.ones(*size, dtype=int16) int16으로 형변환\n",
    "\n",
    "- torch.rand(*size) * n : 주어진 size와 0~n 사이의 랜덤값을 가지는 tensor 생성 \n",
    "- d = b to (torch.int32) : 'b' 텐서의 데이터 타입을 'torch.int32'로 변경하여 새로운 텐서 'd'를 생성\n",
    "  \n",
    "-  'double_d'와 'short_e'라는 두 개의 텐서를 다른 방식으로 생성하고 데이터 타입을 변경하는 여러 가지 방법을 시도함. 이 블록들은 모두 동일한 결과를 생성\n",
    "\n",
    "-  크기가 (5,)인 텐서 'double_f'를 생성하고 데이터 타입을 'torch.double'로 설정함. \n",
    "'double_f'를 'torch.short' 데이터 타입으로 변경하여 'short_g' 텐서를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.add(t1, t2) : 동일한 shape의 tensor 끼리의 합.\n",
    "    - t4 = t1 + t2 : add 함수와 동일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.sub(t1, t2) : 동일한 shape의 tensor 끼리의 차.\n",
    "    - t6 = t1 - t2 : sub함수와 동일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.mul(t1, t2) : 동일한 shape의 tensor 끼리의 곱.\n",
    "    - t8 = t1 * t2 : mul함수와 동일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.div(t1, t2) : 동일한 shape의 tensor 끼리의 몫.\n",
    "    - t10 = t1 / t2 : div함수와 동일."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[ 1.0238, -1.3853],\n",
      "        [ 1.2377, -3.4128]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.dot(tensor([2, 3]), tensor([2, 1])) : 두 tensor의 내적 계산. [2, 3] [2, 1] 이므로 각 성분끼리 곱의 합 = 7\n",
    "- torch.mm(t2, t3) : t2, t3 두 행렬의 곱을 계산.\n",
    "- torch.bmm(t5, t6) : t5, 56 두 tensor 사이의 batch matrix 곱을 계산."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.matmul(t1, t2) : t1, t2의 두 벡터의 내적이 계산됨, 결과는 스칼라 값."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i_tensor_broadcastion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- broadcasting : 서로 다른 모양(shape)을 가진 두 배열(tensor) 간에 요소별 연산을 수행할 때 사용되는 메커니즘.\n",
    "\n",
    "1. 작은 배열이 큰 배열로 확장 : 연산을 수행하기 위해 두 배열의 모양 비교. 작은 배열이 큰 배열의 모양에 맞게 확장\n",
    "2. 요소간 연산 수행 : 브로드케스팅된 배열들은 동일한 모양을 가짐. 요소별 연산을 수행 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 작은 배열이 큰 배열로 확장 -> t4 = tensor([4, 5], [4, 5], [4, 5])로 확장되어 연산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor([2., 2.], [2., 2.]) 으로 확장하여 연산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. normalize 라는 함수 정의 : 주어진 텐서를 255로 나누어 정규화된 값 반환\n",
    "2. 텐서 생성 : 가로 28, 세로 28, 3개의 채널을 가지는 이미지 데이터 생성\n",
    "3. t6 정규화 : normalize(t6)를 통해 255로 나누어서 정규화된 값을 반환 이때 size()는 [3, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t7, t8에서는 t8이 [[3, 1], [3, 1]]로 확장.\n",
    "- t7, t9에서는 t9가 [[5, 5], [2, 2]]로 확장.\n",
    "- t8, t9에서는 두 텐서 모두 size(2, 2)로 확장. 위에 확장된 모습과 동일.\n",
    "- t8, t10에서는 t10이 [[7, 7], [7, 7]]로 확장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 dim끼리 확장을 시킬때 각 dim끼리의 최대공약수로 확장 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t25, t26 dim0이 1로 동일, dim1은 최대공약수인 4로 확장, dim2에서 2와 3이므로 최대공약수가 없어서 확장이 불가 -> 오류."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.pow(input, exponent) : input의 텐서에서 각 요소의 exp제곱하여 반환.\n",
    "- torch.arange(start, end, step(선택), dtype(선택)): start~end까지의 값을 step간격으로 dtype으로 텐서 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic indexing: 인덱스를 지정하여 각 차원에 따라 텐서의 개별 요소에 접근\n",
    "- Slicing: 특정 하위 추출 - 범위를 지정하여 해당 범위에 있는 요소를 선택\n",
    "- Advanced indexing: 일부 라이브러리에서는 Advanced indexing방법 지원. boolean mask를 사용하여 조건을 만족하는 요소를 선택하거나 인덱스 배열을 사용하여 특정 위치에 있는 요소를 선택하는 등이 포함됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.view(*shape) : 원소의 수를 유지하면서 텐서의 크기 변경 --> 참조자를 복사\n",
    "- tensor.reshape(*shape) : view와 반환 결과가 동일하지만, contiguous 속성을 만족하는 텐서에도 적용 가능. --> 값을 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.unsqueeze(dim) : 해당 차원을 확장시키는 함수. 확장된 차원은 0으로 초기화.\n",
    "- tensor.squeeze(dim) : 해당 차원을 삭제시키는 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.flatten(start_dim=0, end_dim=-1) : 차원을 평평하게 만드는 함수 s~e까지 적용\n",
    "- tensor.permute(*dims) : 차원 순서를 바꾸는 함수\n",
    "- tensor.transpose(dim0, dim1) : permute과 유사한 방식으로 작동함. 하지만 transpose는 딱 두개의 차원을 맞교환 가능.\n",
    "- tensor.t() : 2차원 텐서의 행과 열을 바꾸는 연산. 이 함수는 주로 행렬의 전치를 수행하는 데 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7])\n",
      "torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2,7)\n",
    "print(t1.size())\n",
    "t2 = t1.t()\n",
    "print(t2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cat(): concatenate를 해주는 함수 -> 차원의 갯수는 유지되고 해당 차원이 늘어남."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.stack([a,b],dim=0)\n",
    "    - a : [2,3] -> [1,2,3]\n",
    "    - b : [2,3] -> [1,2,3]\n",
    "    - [2,2,3]\n",
    "- torch.stack([a,b],dim=1)\n",
    "    - a : [2,3] -> [2,1,3]\n",
    "    - b : [2,3] -> [2,1,3]\n",
    "    - [2,2,3]\n",
    "- torch.stack([a,b],dim=2)\n",
    "    - a : [2,3] -> [2,3,1]\n",
    "    - b : [2,3] -> [2,3,1]\n",
    "    - [2,3,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.vstack(tensors): 주어진 여러 텐서들을 수직으로 쌓아서 새로운 텐서를 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.hstack(tensors): 주어진 여러 텐서들을 수평으로 쌓아서 새로운 텐서를 생성하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습후기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서와 torch 함수에 대해 여러가지를 공부했습니다.<br>\n",
    "이번 과제를 하면서 배운 내용을 정리해보도록 하겠습니다. <br>\n",
    "\n",
    "- 텐서 생성 및 초기화\n",
    "    - torch.Tensor()\n",
    "    - torch.tensor()\n",
    "    - torch.ones() : 주어진 shape으로 1 초기화\n",
    "    - torch.zeros() : 주어진 shape으로 0 초기화\n",
    "    - torch.empty() : 쓰레기값으로 초기화\n",
    "<br>\n",
    "- 텐서 연산\n",
    "    - sum, sub, mul, div를 포함한 텐서의 여러가지 연산 dot, mm, bmm, matmul 등을 배울 수 있었습니다.\n",
    "<br>\n",
    "- 텐서 변환\n",
    "    - view, reshape : 차원 재구성\n",
    "    - squeeze, unsqueeze : 차원 확장, 축소\n",
    "    - flatten : 차원 평탄화\n",
    "    - permute, transpose, t : 차원 재구성\n",
    "<br>\n",
    "- tensor stacking\n",
    "    - cat() or concat() : 차원 쌓기\n",
    "    - stack() : 새로운 차원으로 확장하여 텐서 시퀀스 병합\n",
    "    - vstack() : 수직으로 쌓아서 새로운 텐서 생성\n",
    "    - hstack() : 수평으로 쌓아서 새로운 텐서 생성\n",
    "\n",
    "이러한 함수들은 딥러닝 모델을 만드는데 많은 역할들을 할 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
